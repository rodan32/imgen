# Vibes ImGen

**Iterative image generation orchestrator** â€” Progressive refinement across 4 GPUs with multi-stage LLM prompt refinement.

## Project Status

### âœ… Backend â€” COMPLETE
- **Core Services**: GPU registry, ComfyUI client pool, task router, workflow engine, image store, WebSocket progress aggregator
- **API Routes**: Sessions, generation (single + batch), GPUs status
- **Database**: SQLAlchemy async + aiosqlite (models + schemas)
- **Workflow Templates**: 8 templates (SD1.5, SDXL, Flux txt2img/img2img, LoRA injection, upscaling)
- **Main App**: FastAPI with lifespan events, CORS, WebSocket endpoint

### ðŸš§ Frontend â€” SCAFFOLDED
- **Build Status**: Compiles clean, runs successfully
- **Draft Grid Flow**: IMPLEMENTED (20-draft funnel with iterative refinement)
- **Placeholder Flows**: Concept Builder, Explorer (UI placeholders only)
- **State Management**: Zustand stores (session, generation, GPU), WebSocket hook
- **Shared Components**: Image grid, prompt editor, GPU status bar, feedback bar

### ðŸ”´ NOT YET BUILT
- **LLM Pipeline**: 3-stage prompt refinement (feedback interpreter â†’ prompt engineer â†’ vision analyzer)
- **Iteration Router**: Feedback engine to generate iteration plans
- **Concept Builder UI**: Structured fields + concept locking
- **Explorer UI**: LoRA browser + A/B compare
- **Video Builder UI**: Keyframe timeline + AnimateDiff rendering
- **Reference Controls**: ControlNet/IP-Adapter injection for pose/style/face preservation

---

## Architecture

```
Browser (React 19 + TS + Tailwind v3 + Vite 5 + Zustand)
    â”‚ HTTP + WebSocket
    â–¼
FastAPI Orchestrator (:8001) @ backend VM
    â”‚  GPU Registry, Task Router, Workflow Engine
    â”‚  (future: Feedback Engine, LLM Refiner, Batch Dispatcher)
    â”‚
    â”œâ”€â”€â–º ComfyUI @ 192.168.0.20:8188 (5060 Ti 16GB â€” premium tier) âœ“ installed
    â”œâ”€â”€â–º ComfyUI @ machine-B:8188 (4060 Ti 8GB â€” quality tier)
    â”œâ”€â”€â–º ComfyUI @ machine-C:8188 (3060 12GB â€” standard tier)
    â””â”€â”€â–º ComfyUI @ machine-D:8188 (3050 Ti 4GB â€” draft tier)

    (future) Ollama @ localhost:11434 (prompt refinement + vision)
```

### Deployment Topology
- **Frontend VM** (Proxmox): Serves built React app (~1 GB RAM, 1 vCPU)
- **Backend VM** (Proxmox): FastAPI orchestrator (~2-4 GB RAM, 1-2 vCPUs, NAS mount)
- **4x GPU Machines** (native): Each runs ComfyUI with `--listen 0.0.0.0`
- **Ollama** (future): On spare GPU machine or dedicated VM
- **NAS** (Synology): Central model repo + generated image storage

---

## Multi-GPU Strategy

| GPU | VRAM | Tier | Capabilities | Role |
|-----|------|------|-------------|------|
| 5060 Ti | 16GB | `premium` | SD1.5, SDXL, Pony, Illustrious, Flux fp8, upscale, ControlNet, IP-Adapter | Final quality, upscaling, reference controls |
| 4060 Ti | 8GB | `quality` | SD1.5, SDXL, Pony, Illustrious | Quality SDXL generation |
| 3060 | 12GB | `standard` | SD1.5, SDXL, Pony, Illustrious | Mid-quality SDXL |
| 3050 Ti | 4GB | `draft` | SD1.5 only | Bulk draft thumbnails (512x512) |

**Routing Strategy**:
- **Batch/Draft Work**: ALL idle GPUs participate â€” faster GPUs complete drafts quicker
- **Quality/Final Work**: Prefer higher-tier GPUs (quality â†’ premium)
- **Overflow Handling**: If preferred tier is overloaded, routes to next-best tier
- **Health Checks**: Background polling (10s interval) marks slow/unresponsive GPUs unhealthy
- **Graceful Degradation**: System works with 1-2 GPUs available (fewer parallel drafts)

GPU config: `config/gpus.yaml` â€” update host/port/capabilities per machine.

---

## Four Generation Flows

### 1. Concept Builder (NOT YET BUILT)
- **Input**: Structured fields (subject, pose, background, style, mood, lighting)
- **Process**: Ollama composes fields â†’ 4 SDXL variations â†’ user selects â†’ LLM refines â†’ repeat
- **Key Feature**: Field locking â€” lock "subject" constant while iterating style/mood
- **UI**: Two-column â€” concept form (left 40%) + image grid (right 60%)

### 2. Draft Grid (Funnel) âœ… IMPLEMENTED
- **Input**: Simple text prompt
- **Process**: 20 rapid SD1.5 drafts â†’ user selects â†’ 8 SDXL refined â†’ 3 polished â†’ 1 final
- **Key Feature**: Progressive quality funnel â€” fewer images, better models/settings each stage
- **UI**: Full-width grid with funnel breadcrumb showing stages
- **Status**: Fully functional â€” prompt â†’ generate â†’ select â†’ advance iteration

### 3. Concept Explorer (NOT YET BUILT)
- **Input**: LoRA selection + optional prompt
- **Process**: Browse LoRAs â†’ generate showcase â†’ refine with feedback â†’ A/B compare combos
- **Key Feature**: LoRA browser with strength sliders, A/B comparison view
- **UI**: Three-column â€” LoRA browser (left) + gallery (center) + controls (right)

### 4. Keyframe Video Builder (NOT YET BUILT)
- **Input**: Scene concept + keyframe descriptions (text/image per keyframe)
- **Process**: Define keyframes iteratively â†’ arrange timeline â†’ preview motion â†’ full AnimateDiff render
- **Key Feature**: Subject consistency via IP-Adapter + ControlNet, iterative keyframe refinement
- **UI**: Horizontal timeline (bottom) + keyframe thumbnails + preview player (center)

---

## Iteration/Feedback System (NOT YET BUILT)

All flows share a common feedback loop:
- **Select/Reject**: Pick favorites, advance the funnel
- **More Like This**: img2img with low denoise (0.3-0.5) using same seed
- **Refine**: User gives text direction â†’ 3-stage LLM pipeline refines prompt
- **Iterate**: Direct parameter adjustments via sliders
- **Upscale**: Send to premium GPU with ESRGAN

### 3-Stage LLM Prompt Refinement Pipeline

**Why multi-stage?** Single-LLM calls drift from intent, add flowery language, don't understand SD syntax.

#### Stage 1: Feedback Interpreter
- **Input**: User's free-text feedback + selected/rejected image IDs
- **Output**: Structured JSON change instructions
- **LLM**: `hermes3:8b` (Ollama) â€” excellent at structured/JSON output
- **Example**: "make it moodier, keep pose, change background to rainy city"
  ```json
  {
    "keep": ["1girl", "sitting cross-legged", "detailed face"],
    "remove": ["forest background", "sunny"],
    "add": ["rainy cityscape background", "neon reflections", "moody atmosphere"],
    "modify": {"lighting": "dim, atmospheric"},
    "emphasis_up": ["mood", "atmosphere"],
    "emphasis_down": []
  }
  ```

#### Stage 2: Prompt Engineer
- **Input**: Current prompt + structured changes from Stage 1 + model family
- **Output**: New SD/Flux-optimized prompt
- **LLM**: `phi4:14b` (Ollama) â€” strong reasoning, precise rule-following
- **System Prompt**: SD prompt syntax rules, model-specific conventions, anti-patterns
- **Key Constraint**: NEVER invents concepts â€” only applies structured changes from Stage 1

#### Stage 3: Vision Analyzer (optional)
- **When Used**: User selects image and says "more like this"
- **Input**: Selected image bytes + question about what to describe
- **Output**: Visual description (composition, style, mood, colors)
- **LLM**: `minicpm-v` (Ollama) â€” multimodal vision model
- **Fed into Stage 2** as additional context

**Session Intent Tracking**:
- Backend maintains "Intent Document" per session â€” accumulates feedback, patterns from rejections, explicit preferences
- Stage 1 receives full history â€” can spot patterns across multiple rounds
- A/B comparison: Vision model compares selected vs rejected to find delta
- Rejection learning: Builds "avoid" list for Stage 2

**Status**: NOT YET IMPLEMENTED â€” services files exist as stubs, Ollama integration pending.

---

## Tech Stack

### Backend
- **Framework**: FastAPI 0.115.0 + uvicorn
- **Database**: SQLAlchemy async + aiosqlite
- **HTTP**: httpx (async ComfyUI client)
- **WebSocket**: websockets 14.1 (progress streaming)
- **Image**: Pillow 11.0 (thumbnails)
- **Config**: PyYAML 6.0

### Frontend
- **Framework**: React 19.0.0 + TypeScript 5.6
- **Build**: Vite 5.4 (NOT Vite 7 â€” requires Node 22+)
- **Styling**: Tailwind CSS v3.4 (NOT v4 â€” different config approach)
- **State**: Zustand 5.0 (minimal boilerplate, fine-grained subscriptions)
- **Node**: v20.12.2 (current on dev machine)

### Image Generation
- **Engine**: 4x ComfyUI instances (native, one per GPU machine)
- **LLM** (future): Ollama (hermes3:8b, phi4:14b, minicpm-v)
- **Models**: SD1.5, SDXL, Flux fp8, ControlNet, IP-Adapter, ESRGAN

---

## File Structure

```
I:\Vibes\ImGen\
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py              â€” FastAPI app entry, lifespan, WS endpoint
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ database.py      â€” SQLAlchemy async session factory
â”‚   â”‚   â”‚   â”œâ”€â”€ orm.py           â€” DB models (SessionORM, GenerationORM)
â”‚   â”‚   â”‚   â””â”€â”€ schemas.py       â€” Pydantic request/response models
â”‚   â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”‚   â”œâ”€â”€ sessions.py      â€” POST/GET /api/sessions
â”‚   â”‚   â”‚   â”œâ”€â”€ generation.py    â€” POST /api/generate, /api/generate/batch
â”‚   â”‚   â”‚   â””â”€â”€ gpus.py          â€” GET /api/gpus
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ gpu_registry.py  â€” GPU config loader, health checks
â”‚   â”‚   â”‚   â”œâ”€â”€ comfyui_client.py â€” HTTP+WS client pool (one per GPU)
â”‚   â”‚   â”‚   â”œâ”€â”€ task_router.py   â€” Route tasks to GPUs by tier/capability
â”‚   â”‚   â”‚   â”œâ”€â”€ workflow_engine.py â€” Load templates, substitute params, inject LoRAs
â”‚   â”‚   â”‚   â””â”€â”€ image_store.py   â€” Filesystem storage + thumbnails
â”‚   â”‚   â”œâ”€â”€ websocket/
â”‚   â”‚   â”‚   â””â”€â”€ aggregator.py    â€” Multiplex ComfyUI WS â†’ per-session frontend WS
â”‚   â”‚   â””â”€â”€ templates/workflows/
â”‚   â”‚       â”œâ”€â”€ manifest.yaml    â€” Workflow template metadata
â”‚   â”‚       â”œâ”€â”€ sd15_txt2img.json
â”‚   â”‚       â”œâ”€â”€ sd15_img2img.json
â”‚   â”‚       â”œâ”€â”€ sdxl_txt2img.json
â”‚   â”‚       â”œâ”€â”€ sdxl_img2img.json
â”‚   â”‚       â”œâ”€â”€ sdxl_with_lora.json
â”‚   â”‚       â”œâ”€â”€ flux_txt2img.json
â”‚   â”‚       â””â”€â”€ upscale_esrgan.json
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.tsx             â€” React entry
â”‚   â”‚   â”œâ”€â”€ App.tsx              â€” Top-level routing (flow selector + active flow)
â”‚   â”‚   â”œâ”€â”€ types/index.ts       â€” TypeScript types
â”‚   â”‚   â”œâ”€â”€ api/client.ts        â€” HTTP API client
â”‚   â”‚   â”œâ”€â”€ stores/
â”‚   â”‚   â”‚   â”œâ”€â”€ sessionStore.ts  â€” Current session, stage, iteration round
â”‚   â”‚   â”‚   â”œâ”€â”€ generationStore.ts â€” Generation results, selection state
â”‚   â”‚   â”‚   â””â”€â”€ gpuStore.ts      â€” GPU status polling
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â””â”€â”€ useWebSocket.ts  â€” WebSocket connection + message handling
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ flows/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ FlowSelector.tsx  â€” Initial flow picker
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ DraftGrid/
â”‚   â”‚   â”‚   â”‚       â””â”€â”€ DraftGridFlow.tsx â€” âœ… Draft Grid implementation
â”‚   â”‚   â”‚   â””â”€â”€ shared/
â”‚   â”‚   â”‚       â”œâ”€â”€ GPUStatusBar.tsx
â”‚   â”‚   â”‚       â”œâ”€â”€ ImageGrid.tsx
â”‚   â”‚   â”‚       â”œâ”€â”€ ImageCard.tsx
â”‚   â”‚   â”‚       â”œâ”€â”€ PromptEditor.tsx
â”‚   â”‚   â”‚       â””â”€â”€ FeedbackBar.tsx
â”‚   â”‚   â””â”€â”€ index.css            â€” Tailwind imports + custom CSS vars
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.ts
â”‚   â”œâ”€â”€ tailwind.config.js       â€” Tailwind v3 config (custom color palette)
â”‚   â””â”€â”€ tsconfig.json
â”œâ”€â”€ config/
â”‚   â””â”€â”€ gpus.yaml                â€” GPU node registry (host/port/capabilities)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ images/                  â€” Generated images
â”‚   â””â”€â”€ uploads/                 â€” User-uploaded reference images
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ setup_comfyui.py         â€” Automated ComfyUI setup script (per tier)
â””â”€â”€ README.md                     â€” This file
```

---

## Key Patterns

### Backend
- **ComfyUIClientPool**: One HTTP+WS client per GPU node, initialized at startup
- **TaskRouter**: Tier-based routing with overflow, health-aware
- **WorkflowEngine**: JSON templates with `{{variable}}` placeholders, dynamic LoRA injection
- **ProgressAggregator**: Multiplexes ComfyUI progress â†’ per-session frontend WebSocket
- **Background Tasks**: `asyncio.create_task` for generation jobs (300s timeout)
- **Database**: Async SQLAlchemy with aiosqlite, session factory in `app.state.db_session`

### Frontend
- **Zustand Stores**: `sessionStore` (session state), `generationStore` (results + selections), `gpuStore` (GPU status)
- **WebSocket Hook**: `useWebSocket(sessionId)` â€” connects, handles progress + complete messages, updates stores
- **API Client**: `src/api/client.ts` â€” typed fetch wrappers for all endpoints
- **Shared Components**: Image grid (lazy), image card (select/reject), prompt editor, GPU status bar

---

## Current Configuration

### GPU Nodes (`config/gpus.yaml`)
```yaml
nodes:
  - id: gpu-premium
    name: "RTX 5060 Ti"
    vram_gb: 16
    tier: premium
    host: "192.168.0.20"  # âœ“ installed via Stability Matrix
    port: 8188
    capabilities: [sd15, sdxl, pony, illustrious, flux, flux_fp8, upscale, controlnet, ipadapter, faceid]
    max_resolution: 1536
    max_batch: 4

  - id: gpu-quality
    name: "RTX 4060 Ti"
    vram_gb: 8
    tier: quality
    host: "192.168.1.101"  # UPDATE: IP of 4060 Ti machine
    port: 8188
    capabilities: [sd15, sdxl, pony, illustrious]
    max_resolution: 1024
    max_batch: 4

  - id: gpu-standard
    name: "RTX 3060"
    vram_gb: 12
    tier: standard
    host: "192.168.1.102"  # UPDATE: IP of 3060 machine
    port: 8188
    capabilities: [sd15, sdxl, pony, illustrious]
    max_resolution: 1024
    max_batch: 2

  - id: gpu-draft
    name: "RTX 3050 Ti"
    vram_gb: 4
    tier: draft
    host: "192.168.1.103"  # UPDATE: IP of 3050 Ti machine
    port: 8188
    capabilities: [sd15]
    max_resolution: 512
    max_batch: 1
```

**Action Required**: Update IP addresses for machines B, C, D once ComfyUI is installed.

### Workflow Templates (`backend/app/templates/workflows/`)
8 templates currently implemented:
- **sd15_txt2img**: SD 1.5 text-to-image (fast drafts, 512x512, 10 steps)
- **sd15_img2img**: SD 1.5 image-to-image (15 steps, denoise 0.5)
- **sdxl_txt2img**: SDXL text-to-image (1024x1024, 25 steps)
- **sdxl_img2img**: SDXL image-to-image (25 steps, denoise 0.5)
- **sdxl_with_lora**: SDXL with LoRA support (max 3 LoRAs)
- **flux_txt2img**: Flux text-to-image (1024x1024, 20 steps, cfg 1.0)
- **upscale_esrgan**: ESRGAN 4x upscale (any model family)

Each template is ComfyUI API-format JSON with `{{prompt}}`, `{{checkpoint}}`, etc. placeholders.

---

## Running the Project

### Prerequisites
1. **ComfyUI installed on each GPU machine** with `--listen 0.0.0.0` flag
2. **Firewall rules** allowing port 8188 inbound on each machine
3. **Models downloaded** per tier (see plan file for download links)
4. **Python 3.11+** on backend VM
5. **Node v20.12.2** on frontend dev machine (or VM)

### Backend Setup
```bash
cd backend
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt

# Update config/gpus.yaml with actual IP addresses

# Initialize database (auto-creates on first run)
uvicorn app.main:app --host 0.0.0.0 --port 8001
```

Backend runs at `http://localhost:8001`
- Health check: `GET /health`
- GPU status: `GET /api/gpus`
- API docs: `http://localhost:8001/docs` (FastAPI auto-generated)

### Frontend Setup
```bash
cd frontend
npm install
npm run dev
```

Frontend runs at `http://localhost:5173` (Vite default)

**Production Build**:
```bash
npm run build  # outputs to frontend/dist/
```

Serve `dist/` via Nginx/Caddy on frontend VM.

---

## API Endpoints

### Sessions
- `POST /api/sessions` â€” Create new session
  - Body: `{ "flow_type": "draft_grid", "initial_config": {...} }`
  - Returns: `{ "id": "uuid", "flow_type": "draft_grid", "created_at": "..." }`
- `GET /api/sessions/{id}` â€” Load existing session
- `GET /api/sessions` â€” List all sessions (pagination TODO)

### Generation
- `POST /api/generate` â€” Single image generation
  - Body: `GenerationRequest` (prompt, model_family, task_type, params, session_id)
  - Returns: `GenerationResponse` (id, status, gpu_id)
- `POST /api/generate/batch` â€” Batch generation (distributed across GPUs)
  - Body: `BatchGenerationRequest` (prompt, count, params)
  - Returns: `BatchGenerationResponse` (batch_id, total_count, gpu_assignments)
- `GET /api/generate/{id}` â€” Get generation result
- `GET /api/images/{session_id}/{image_id}` â€” Serve generated image
- `GET /api/images/{session_id}/{image_id}/thumb` â€” Serve thumbnail

### GPUs
- `GET /api/gpus` â€” List all GPU nodes with health status

### WebSocket
- `WS /ws/session/{session_id}` â€” Real-time progress streaming
  - Receives: `{ "type": "progress", "generation_id": "...", "current": 5, "total": 20 }`
  - Receives: `{ "type": "complete", "generation_id": "...", "image_url": "...", "thumbnail_url": "..." }`

### Future Endpoints (NOT YET IMPLEMENTED)
- `POST /api/iterate` â€” Submit feedback, get iteration plan
- `POST /api/iterate/auto` â€” Submit feedback + auto-generate
- `POST /api/iterate/refine-prompt` â€” LLM prompt refinement
- `POST /api/iterate/concept` â€” Concept Builder structured generation
- `GET /api/models`, `/api/loras` â€” Available checkpoints and LoRAs
- `POST /api/video/keyframes`, `GET /api/video/{id}/timeline`, etc. (Video Builder)

---

## ComfyUI Setup (Per Machine)

### Installation
**Recommended**: Stability Matrix (current approach on 5060 Ti @ `192.168.0.20`)
1. Install ComfyUI via Stability Matrix
2. **Enable LAN access**: Add `--listen 0.0.0.0` to launch arguments in package settings
3. Open Windows Firewall for port 8188 (inbound rule, TCP)
4. Restart ComfyUI â€” verify reachable from other machines

**Alternative** (manual):
```bash
git clone https://github.com/Comfy-Org/ComfyUI
cd ComfyUI
# Python setup (venv + deps)
python main.py --listen 0.0.0.0 --port 8188
```

### Custom Nodes (all machines)
- **ComfyUI Manager**: Makes installing other nodes easy
- **ComfyUI_IPAdapter_plus**: `git clone https://github.com/cubiq/ComfyUI_IPAdapter_plus.git` in `custom_nodes/`
- **insightface** (for FaceID): `pip install insightface` (needs Visual C++ build tools on Windows)
- **(Future) ComfyUI-AnimateDiff-Evolved**: For video keyframe flow
- **(Future) ComfyUI-Frame-Interpolation**: RIFE/FILM interpolation
- **(Future) ComfyUI-VideoHelperSuite**: Video I/O

### Model Downloads (per tier)

**3050 Ti (draft tier, ~4 GB)**:
- SD 1.5: `v1-5-pruned-emaonly.safetensors` (~4 GB) â†’ `models/checkpoints/`

**3060 / 4060 Ti (standard/quality tier, ~10 GB)**:
- SD 1.5: `v1-5-pruned-emaonly.safetensors` (~4 GB)
- SDXL Base: `sd_xl_base_1.0.safetensors` (~6.1 GB)

**5060 Ti (premium tier, ~40+ GB)**:
- SD 1.5: `v1-5-pruned-emaonly.safetensors` (~4 GB)
- SDXL Base: `sd_xl_base_1.0.safetensors` (~6.1 GB)
- Flux Dev FP8: `flux1-dev-fp8.safetensors` (~12 GB) â†’ [Comfy-Org/flux1-dev](https://huggingface.co/Comfy-Org/flux1-dev)
- ControlNet models (OpenPose, Depth, Canny): ~6.5 GB total â†’ `models/controlnet/`
- IP-Adapter models (Plus, FaceID PlusV2): ~1.6 GB total â†’ `models/ipadapter/`
- CLIP Vision: `CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors` (~2.5 GB) â†’ `models/clip_vision/`
- Upscaler: `4x-UltraSharp.pth` (~67 MB) â†’ `models/upscale_models/`

Full download links in plan file: `C:\Users\zcoch\.claude\plans\velvet-percolating-quiche.md`

---

## Verification Checklist

### Backend
- [ ] ComfyUI running on each machine with `--listen 0.0.0.0`
- [ ] `config/gpus.yaml` updated with actual IP addresses
- [ ] Backend starts without errors: `uvicorn app.main:app --port 8001`
- [ ] Health check passes: `curl http://localhost:8001/health`
- [ ] GPU status shows all nodes: `curl http://localhost:8001/api/gpus`
- [ ] Initial health check logs show all GPUs healthy (or identifies unreachable ones)

### Frontend
- [ ] Frontend builds clean: `npm run build`
- [ ] Dev server runs: `npm run dev`
- [ ] Can select Draft Grid flow from menu
- [ ] Prompt editor appears, can type prompt
- [ ] GPU status bar shows GPU nodes

### Integration
- [ ] Create session via API: `POST /api/sessions`
- [ ] Generate single image: `POST /api/generate` (verify routes to correct GPU)
- [ ] Generate batch: `POST /api/generate/batch` (verify distribution across GPUs)
- [ ] WebSocket connects: Check browser console for WS connection
- [ ] Draft Grid flow end-to-end: prompt â†’ generate 20 drafts â†’ drafts appear in grid
- [ ] Select images â†’ click "Continue with Selected" â†’ iteration advances (prompt updates)

### Future (NOT YET TESTABLE)
- [ ] LLM refinement: Provide feedback text â†’ verify Ollama produces refined prompt
- [ ] img2img: Select image â†’ "more like this" â†’ verify img2img generation
- [ ] ControlNet: Upload reference â†’ "keep this pose" â†’ verify pose transfer
- [ ] Video: Create keyframes â†’ preview â†’ full render â†’ verify subject consistency

---

## Known Gotchas

1. **Node Version**: v20.12.2 required â€” Vite 5 compatible. Vite 7+ needs Node 22+.
2. **Tailwind Version**: v3.4 used (NOT v4) â€” v4 has different config approach.
3. **Zustand Version**: v5.0 â€” uses `create()` API, no middleware needed for basic stores.
4. **ComfyUI API**: Always enabled by default â€” web UI endpoint IS the API endpoint.
5. **Firewall**: Windows Firewall blocks port 8188 by default â€” must create inbound rule.
6. **poll_until_complete**: Has 300s (5 min) timeout hardcoded in backend.
7. **Workflow Templates**: Use `{{variable}}` syntax, NOT Jinja2 `{{ variable }}`.
8. **Image Paths**: Served via `/api/images/{session_id}/{image_id}`, NOT static file serving.

---

## Next Steps (Implementation Order)

1. **Verify Current Setup**:
   - Install ComfyUI on machines B, C, D
   - Update `config/gpus.yaml` with actual IPs
   - Run backend + frontend, test Draft Grid flow end-to-end

2. **LLM Pipeline** (Priority 1):
   - Implement `feedback_interpreter.py`, `prompt_engineer.py`, `vision_analyzer.py`
   - Integrate Ollama (install, download models: hermes3:8b, phi4:14b, minicpm-v)
   - Add API routes: `POST /api/iterate/refine-prompt`
   - Test: User feedback â†’ structured changes â†’ refined prompt

3. **Iteration System** (Priority 2):
   - Implement `FeedbackEngine` to generate `IterationPlan`
   - Add API routes: `POST /api/iterate`, `POST /api/iterate/auto`
   - Frontend: Feedback bar with "More Like This", "Refine", "Iterate" buttons
   - Test: Select image â†’ feedback â†’ iteration plan â†’ auto-generate

4. **Concept Builder Flow** (Priority 3):
   - Frontend UI: Structured form (subject, pose, background, style, mood, lighting)
   - Field locking UI (lock icon per field)
   - Ollama prompt composition from fields
   - Test: Fill form â†’ generate variations â†’ lock fields â†’ iterate

5. **Explorer Flow** (Priority 4):
   - LoRA discovery: Query ComfyUI instances for installed LoRAs
   - LoRA browser UI (search, filter, strength sliders)
   - A/B comparison view (side-by-side with diff overlay)
   - Test: Select LoRA â†’ generate showcase â†’ compare variants

6. **Reference Controls** (Priority 5):
   - ControlNet/IP-Adapter workflow injection in `WorkflowEngine`
   - Frontend: Upload reference image, select control type (pose/style/face)
   - Pinned traits system (chips in UI, persistence in Intent Document)
   - Test: Upload face â†’ "keep this face" â†’ generate â†’ verify face preserved

7. **Video Builder Flow** (Priority 6):
   - Timeline UI (horizontal, keyframe thumbnails)
   - Keyframe editor (add/remove/reorder)
   - AnimateDiff workflow templates + custom nodes
   - RIFE/FILM interpolation
   - Test: Create 3 keyframes â†’ preview â†’ full render â†’ verify consistency

8. **Polish + Deployment**:
   - Flux templates (currently have txt2img, add img2img)
   - ESRGAN upscaling integration (workflow exists, test E2E)
   - Lightbox for full-size image viewing
   - GPU dashboard (utilization graphs, queue depths)
   - Session management (list, delete, disk usage tracking)
   - Deploy frontend VM + backend VM on Proxmox

---

## Memory File

Project-specific memory maintained at:
`C:\Users\zcoch\.claude\projects\I--Vibes-ImGen\memory\MEMORY.md`

Comprehensive build plan (426 lines):
`C:\Users\zcoch\.claude\plans\velvet-percolating-quiche.md`

---

## License

Private project â€” no public license.

---

**Last Updated**: 2026-02-09
**Project Phase**: Backend complete, Frontend scaffolded, Draft Grid flow working
**Next Milestone**: LLM pipeline implementation (3-stage prompt refinement)
